// Package parser implements a PostgreSQL-compatible SQL lexer and parser.
// This lexer is a direct translation of PostgreSQL's scan.l.
package parser

import (
	"fmt"
	"strconv"
	"strings"
	"unicode"
	"unicode/utf8"
)

// Token types returned by the lexer (internal, prefixed with lex_).
// These are mapped to parser token constants in parse.go.
// Keywords are defined in keywords.go using constants from parser.go.
// Non-keyword tokens start after all keywords to avoid conflicts.
// Base token value for keywords (must be > 256 to avoid ASCII conflicts)
// This value is used in the lexer for keyword token recognition.
// The actual keyword constants are generated by goyacc in parser.go.
const keywordTokenBase = 258

// Non-keyword token base - must be after all keyword tokens
// Keywords use 258 + 0..491 (492 keywords), so we start at 800
const nonKeywordTokenBase = 800

const (
	// Special tokens
	lex_EOF = 0
)

// Internal lexer token types for non-keywords.
// These start at nonKeywordTokenBase and are mapped to parser constants in parse.go.
const (
	lex_ICONST = nonKeywordTokenBase + iota
	lex_FCONST
	lex_SCONST  // String constant
	lex_BCONST  // Bit string constant (B'...')
	lex_XCONST  // Hex string constant (X'...')
	lex_USCONST // Unicode string constant (U&'...')
	lex_IDENT
	lex_UIDENT // Unicode identifier (U&"...")
	lex_TYPECAST       // ::
	lex_DOT_DOT        // ..
	lex_COLON_EQUALS   // :=
	lex_EQUALS_GREATER // =>
	lex_LESS_EQUALS    // <=
	lex_GREATER_EQUALS // >=
	lex_NOT_EQUALS     // <> or !=
	lex_PARAM // $1, $2, etc.
	lex_Op
)

// LexerState represents the current state of the lexer (for string/comment handling).
type LexerState int

const (
	stateInitial LexerState = iota
	stateXB      // bit string literal
	stateXC      // extended C-style comments
	stateXD      // delimited identifiers (double-quoted)
	stateXH      // hexadecimal byte string
	stateXQ      // standard quoted strings
	stateXQS     // quote stop (detect continued strings)
	stateXE      // extended quoted strings (backslash escapes)
	stateXDOLQ   // dollar-quoted strings
	stateXUI     // quoted identifier with Unicode escapes
	stateXUS     // quoted string with Unicode escapes
	stateXEU     // Unicode surrogate pair in extended quoted string
)

// Token represents a lexical token.
type Token struct {
	Type   int    // Token type (IDENT, ICONST, keyword token, etc.)
	Str    string // String value for identifiers, operators, string literals
	Ival   int64  // Integer value for ICONST
	Loc    int    // Byte offset in the source text
}

// Lexer implements a PostgreSQL-compatible SQL lexer.
type Lexer struct {
	input    string // Input SQL text
	pos      int    // Current position in input (byte offset)
	start    int    // Start position of current token

	state    LexerState // Current lexer state
	stateBeforeStrStop LexerState // State before entering xqs

	// Literal buffer for building string/identifier values
	literalbuf strings.Builder

	// Dollar quote delimiter
	dolqstart string

	// Unicode surrogate handling
	utf16FirstPart rune

	// Comment nesting depth
	xcdepth int

	// Configuration
	StandardConformingStrings bool // If true, backslash is not escape in standard strings
	BackslashQuote            int  // 0=off, 1=on, 2=safe_encoding
	EscapeStringWarning       bool

	// Warning tracking
	warnOnFirstEscape bool
	sawNonASCII       bool

	// Location tracking
	savedLoc int

	// Error handling
	Err error
}

// BackslashQuote values
const (
	BackslashQuoteOff = iota
	BackslashQuoteOn
	BackslashQuoteSafeEncoding
)

// NewLexer creates a new lexer for the given input.
func NewLexer(input string) *Lexer {
	return &Lexer{
		input:                     input,
		pos:                       0,
		state:                     stateInitial,
		StandardConformingStrings: true,
		BackslashQuote:            BackslashQuoteSafeEncoding,
		EscapeStringWarning:       true,
	}
}

// NextToken returns the next token from the input.
func (l *Lexer) NextToken() Token {
	for {
		if l.pos >= len(l.input) {
			// At EOF - if we're in a string state, that's an error
			// If we're in quote-stop state, return the completed string
			if l.state == stateXQS {
				return l.lexQuoteContinue()
			}
			return Token{Type: lex_EOF, Loc: l.pos}
		}

		switch l.state {
		case stateInitial:
			l.start = l.pos
			return l.lexInitial()
		case stateXC:
			l.lexComment()
			continue
		case stateXB, stateXH:
			return l.lexBitOrHexString()
		case stateXQ, stateXE, stateXUS:
			return l.lexQuotedString()
		case stateXQS:
			return l.lexQuoteContinue()
		case stateXEU:
			return l.lexUnicodeSurrogate()
		case stateXD, stateXUI:
			return l.lexDelimitedIdent()
		case stateXDOLQ:
			return l.lexDollarQuote()
		default:
			l.Err = fmt.Errorf("unknown lexer state: %d", l.state)
			return Token{Type: lex_EOF, Loc: l.start}
		}
	}
}

// lexInitial handles tokens in the initial state.
func (l *Lexer) lexInitial() Token {
	l.skipWhitespace()
	if l.pos >= len(l.input) {
		return Token{Type: lex_EOF, Loc: l.pos}
	}

	l.start = l.pos
	ch := l.input[l.pos]

	// Check for C-style comment start
	if ch == '/' && l.pos+1 < len(l.input) && l.input[l.pos+1] == '*' {
		l.pos += 2
		l.xcdepth = 0
		l.state = stateXC
		// Return to main loop to handle comment
		return l.NextToken()
	}

	// Check for SQL comment
	if ch == '-' && l.pos+1 < len(l.input) && l.input[l.pos+1] == '-' {
		l.skipLineComment()
		return l.NextToken()
	}

	// Bit string: B'...'
	if (ch == 'b' || ch == 'B') && l.pos+1 < len(l.input) && l.input[l.pos+1] == '\'' {
		l.pos += 2
		l.state = stateXB
		l.literalbuf.Reset()
		l.literalbuf.WriteByte('b')
		return l.NextToken()
	}

	// Hex string: X'...'
	if (ch == 'x' || ch == 'X') && l.pos+1 < len(l.input) && l.input[l.pos+1] == '\'' {
		l.pos += 2
		l.state = stateXH
		l.literalbuf.Reset()
		l.literalbuf.WriteByte('x')
		return l.NextToken()
	}

	// National character: N'...' -> treat as NCHAR followed by string
	if (ch == 'n' || ch == 'N') && l.pos+1 < len(l.input) && l.input[l.pos+1] == '\'' {
		// Look up NCHAR keyword
		kw := LookupKeyword("nchar")
		if kw != nil {
			l.pos++ // consume only 'n'
			return Token{Type: kw.Token, Str: kw.Name, Loc: l.start}
		}
		// If NCHAR isn't a keyword, return 'n' as identifier
		l.pos++
		return Token{Type: lex_IDENT, Str: "n", Loc: l.start}
	}

	// Extended quoted string: E'...'
	if (ch == 'e' || ch == 'E') && l.pos+1 < len(l.input) && l.input[l.pos+1] == '\'' {
		l.pos += 2
		l.state = stateXE
		l.warnOnFirstEscape = false
		l.sawNonASCII = false
		l.literalbuf.Reset()
		return l.NextToken()
	}

	// Unicode string: U&'...'
	if (ch == 'u' || ch == 'U') && l.pos+1 < len(l.input) && l.input[l.pos+1] == '&' {
		if l.pos+2 < len(l.input) && l.input[l.pos+2] == '\'' {
			if !l.StandardConformingStrings {
				l.Err = fmt.Errorf("unsafe use of string constant with Unicode escapes")
				return Token{Type: lex_EOF, Loc: l.start}
			}
			l.pos += 3
			l.state = stateXUS
			l.literalbuf.Reset()
			return l.NextToken()
		}
		// Unicode identifier: U&"..."
		if l.pos+2 < len(l.input) && l.input[l.pos+2] == '"' {
			l.pos += 3
			l.state = stateXUI
			l.literalbuf.Reset()
			return l.NextToken()
		}
		// Just 'U&' without quote - return 'u' as identifier
		l.pos++
		ident := l.downcase("u")
		return Token{Type: lex_IDENT, Str: ident, Loc: l.start}
	}

	// Standard quoted string: '...'
	if ch == '\'' {
		l.pos++
		l.warnOnFirstEscape = true
		l.sawNonASCII = false
		if l.StandardConformingStrings {
			l.state = stateXQ
		} else {
			l.state = stateXE
		}
		l.literalbuf.Reset()
		return l.NextToken()
	}

	// Double-quoted identifier: "..."
	if ch == '"' {
		l.pos++
		l.state = stateXD
		l.literalbuf.Reset()
		return l.NextToken()
	}

	// Dollar-quoted string: $tag$...$tag$
	if ch == '$' {
		if tag := l.scanDollarTag(); tag != "" {
			l.dolqstart = tag
			l.state = stateXDOLQ
			l.literalbuf.Reset()
			return l.NextToken()
		}
		// Check for parameter: $1, $2, etc.
		if l.pos+1 < len(l.input) && isDigit(l.input[l.pos+1]) {
			return l.lexParam()
		}
		// Single $ is returned as itself
		l.pos++
		return Token{Type: int(ch), Str: string(ch), Loc: l.start}
	}

	// Two-character special tokens (must check before operators)
	if l.pos+1 < len(l.input) {
		ch2 := l.input[l.pos : l.pos+2]
		switch ch2 {
		case "::":
			l.pos += 2
			return Token{Type: lex_TYPECAST, Str: "::", Loc: l.start}
		case "..":
			l.pos += 2
			return Token{Type: lex_DOT_DOT, Str: "..", Loc: l.start}
		case ":=":
			l.pos += 2
			return Token{Type: lex_COLON_EQUALS, Str: ":=", Loc: l.start}
		}
	}

	// Operators (including multi-char operators like ->, ||, etc.)
	// Must check before self-delimiting because some chars overlap
	if isOpChar(ch) {
		return l.lexOperator()
	}

	// Numbers - check for leading dot numeric BEFORE self-delimiting
	// because .5 should be parsed as a numeric, not as . followed by 5
	if ch == '.' && l.pos+1 < len(l.input) && isDigit(l.input[l.pos+1]) {
		return l.lexNumber()
	}

	// Self-delimiting characters that are NOT operator chars
	// Note: +, -, *, /, %, ^, <, >, = are handled by lexOperator above
	if isSelfOnly(ch) {
		l.pos++
		return Token{Type: int(ch), Str: string(ch), Loc: l.start}
	}

	// Numbers starting with digit
	if isDigit(ch) {
		return l.lexNumber()
	}

	// Identifiers and keywords
	if isIdentStart(ch) {
		return l.lexIdent()
	}

	// Any other character
	l.pos++
	return Token{Type: int(ch), Str: string(ch), Loc: l.start}
}

// skipWhitespace skips whitespace characters.
func (l *Lexer) skipWhitespace() {
	for l.pos < len(l.input) {
		ch := l.input[l.pos]
		if ch == ' ' || ch == '\t' || ch == '\n' || ch == '\r' || ch == '\f' || ch == '\v' {
			l.pos++
		} else if ch == '-' && l.pos+1 < len(l.input) && l.input[l.pos+1] == '-' {
			l.skipLineComment()
		} else {
			break
		}
	}
}

// skipLineComment skips a -- comment to end of line.
func (l *Lexer) skipLineComment() {
	l.pos += 2 // skip --
	for l.pos < len(l.input) {
		ch := l.input[l.pos]
		l.pos++
		if ch == '\n' || ch == '\r' {
			break
		}
	}
}

// lexComment handles C-style comments /* ... */
func (l *Lexer) lexComment() {
	for l.pos < len(l.input) {
		ch := l.input[l.pos]

		// Check for nested comment start
		if ch == '/' && l.pos+1 < len(l.input) && l.input[l.pos+1] == '*' {
			l.xcdepth++
			l.pos += 2
			continue
		}

		// Check for comment end
		if ch == '*' && l.pos+1 < len(l.input) && l.input[l.pos+1] == '/' {
			l.pos += 2
			if l.xcdepth <= 0 {
				l.state = stateInitial
				return
			}
			l.xcdepth--
			continue
		}

		l.pos++
	}

	// Reached EOF inside comment
	l.Err = fmt.Errorf("unterminated /* comment")
	l.state = stateInitial
}

// lexBitOrHexString handles B'...' and X'...' strings.
func (l *Lexer) lexBitOrHexString() Token {
	for l.pos < len(l.input) {
		ch := l.input[l.pos]
		if ch == '\'' {
			l.pos++
			l.stateBeforeStrStop = l.state
			l.state = stateXQS
			return l.NextToken()
		}
		l.literalbuf.WriteByte(ch)
		l.pos++
	}

	if l.state == stateXB {
		l.Err = fmt.Errorf("unterminated bit string literal")
	} else {
		l.Err = fmt.Errorf("unterminated hexadecimal string literal")
	}
	l.state = stateInitial
	return Token{Type: lex_EOF, Loc: l.start}
}

// lexQuotedString handles '...' E'...' and U&'...' strings.
func (l *Lexer) lexQuotedString() Token {
	for l.pos < len(l.input) {
		ch := l.input[l.pos]

		// End quote - check for doubled quote first
		if ch == '\'' {
			// Check for doubled quote (escaped single quote)
			if l.pos+1 < len(l.input) && l.input[l.pos+1] == '\'' {
				l.literalbuf.WriteByte('\'')
				l.pos += 2
				continue
			}
			// Single quote - end of string, check for continuation
			l.pos++
			l.stateBeforeStrStop = l.state
			l.state = stateXQS
			return l.NextToken()
		}

		// For extended strings, handle escape sequences
		if l.state == stateXE && ch == '\\' && l.pos+1 < len(l.input) {
			l.pos++
			l.handleEscapeSequence()
			continue
		}

		l.literalbuf.WriteByte(ch)
		l.pos++
	}

	l.Err = fmt.Errorf("unterminated quoted string")
	l.state = stateInitial
	return Token{Type: lex_EOF, Loc: l.start}
}

// handleEscapeSequence processes escape sequences in E'...' strings.
func (l *Lexer) handleEscapeSequence() {
	if l.pos >= len(l.input) {
		return
	}

	ch := l.input[l.pos]
	l.pos++

	switch ch {
	case 'b':
		l.literalbuf.WriteByte('\b')
	case 'f':
		l.literalbuf.WriteByte('\f')
	case 'n':
		l.literalbuf.WriteByte('\n')
	case 'r':
		l.literalbuf.WriteByte('\r')
	case 't':
		l.literalbuf.WriteByte('\t')
	case 'v':
		l.literalbuf.WriteByte('\v')
	case '0', '1', '2', '3', '4', '5', '6', '7':
		// Octal escape: \0 through \777
		l.pos--
		val := l.scanOctal()
		l.literalbuf.WriteByte(byte(val))
		if val > 127 {
			l.sawNonASCII = true
		}
	case 'x':
		// Hex escape: \xNN
		if l.pos < len(l.input) && isHexDigit(l.input[l.pos]) {
			val := l.scanHex(2)
			l.literalbuf.WriteByte(byte(val))
			if val > 127 {
				l.sawNonASCII = true
			}
		} else {
			l.literalbuf.WriteByte('x')
		}
	case 'u':
		// Unicode escape: \uXXXX
		if l.pos+3 < len(l.input) {
			val := l.scanHex(4)
			l.writeUnicodeChar(rune(val))
		} else {
			l.literalbuf.WriteByte('u')
		}
	case 'U':
		// Unicode escape: \UXXXXXXXX
		if l.pos+7 < len(l.input) {
			val := l.scanHex(8)
			l.writeUnicodeChar(rune(val))
		} else {
			l.literalbuf.WriteByte('U')
		}
	default:
		l.literalbuf.WriteByte(ch)
	}
}

// scanOctal scans an octal escape sequence.
func (l *Lexer) scanOctal() int {
	val := 0
	for i := 0; i < 3 && l.pos < len(l.input); i++ {
		ch := l.input[l.pos]
		if ch < '0' || ch > '7' {
			break
		}
		val = val*8 + int(ch-'0')
		l.pos++
	}
	return val
}

// scanHex scans n hex digits.
func (l *Lexer) scanHex(n int) int {
	val := 0
	for i := 0; i < n && l.pos < len(l.input); i++ {
		ch := l.input[l.pos]
		if !isHexDigit(ch) {
			break
		}
		val = val*16 + hexValue(ch)
		l.pos++
	}
	return val
}

// writeUnicodeChar writes a Unicode code point to the literal buffer.
func (l *Lexer) writeUnicodeChar(r rune) {
	var buf [4]byte
	n := utf8.EncodeRune(buf[:], r)
	l.literalbuf.Write(buf[:n])
}

// lexQuoteContinue handles quote continuation (string literal spanning lines).
func (l *Lexer) lexQuoteContinue() Token {
	// Check if there's whitespace with newline followed by quote
	// SQL requires at least one newline in the whitespace to continue a string
	hasNewline := false

	for l.pos < len(l.input) {
		ch := l.input[l.pos]
		if ch == ' ' || ch == '\t' || ch == '\f' || ch == '\v' {
			l.pos++
		} else if ch == '\n' || ch == '\r' {
			hasNewline = true
			l.pos++
		} else if ch == '-' && l.pos+1 < len(l.input) && l.input[l.pos+1] == '-' {
			// SQL comment - after comment there's an implicit newline
			l.skipLineComment()
			hasNewline = true
		} else {
			break
		}
	}

	// If we see a quote after whitespace with newline, continue the string
	if hasNewline && l.pos < len(l.input) && l.input[l.pos] == '\'' {
		l.pos++
		l.state = l.stateBeforeStrStop
		return l.NextToken()
	}

	// No continuation - return the completed string
	// Don't rewind position, just return from current spot
	l.state = stateInitial

	str := l.literalbuf.String()

	switch l.stateBeforeStrStop {
	case stateXB:
		return Token{Type: lex_BCONST, Str: str, Loc: l.start}
	case stateXH:
		return Token{Type: lex_XCONST, Str: str, Loc: l.start}
	case stateXQ, stateXE:
		return Token{Type: lex_SCONST, Str: str, Loc: l.start}
	case stateXUS:
		return Token{Type: lex_USCONST, Str: str, Loc: l.start}
	default:
		return Token{Type: lex_SCONST, Str: str, Loc: l.start}
	}
}

// lexUnicodeSurrogate handles Unicode surrogate pairs in E'...' strings.
func (l *Lexer) lexUnicodeSurrogate() Token {
	// Looking for the second part of a surrogate pair
	if l.pos+1 < len(l.input) && l.input[l.pos] == '\\' {
		l.pos++
		ch := l.input[l.pos]
		if ch == 'u' || ch == 'U' {
			l.pos++
			n := 4
			if ch == 'U' {
				n = 8
			}
			val := l.scanHex(n)
			// Combine surrogate pair
			if isUTF16SurrogateSecond(rune(val)) {
				combined := surrogateToCodepoint(l.utf16FirstPart, rune(val))
				l.writeUnicodeChar(combined)
				l.state = stateXE
				return l.NextToken()
			}
		}
	}

	l.Err = fmt.Errorf("invalid Unicode surrogate pair")
	l.state = stateInitial
	return Token{Type: lex_EOF, Loc: l.start}
}

// lexDelimitedIdent handles "..." identifiers.
func (l *Lexer) lexDelimitedIdent() Token {
	for l.pos < len(l.input) {
		ch := l.input[l.pos]

		// End quote
		if ch == '"' {
			l.pos++
			// Check for "" (escaped quote)
			if l.pos < len(l.input) && l.input[l.pos] == '"' {
				l.literalbuf.WriteByte('"')
				l.pos++
				continue
			}
			// End of identifier
			l.state = stateInitial
			str := l.literalbuf.String()
			if str == "" {
				l.Err = fmt.Errorf("zero-length delimited identifier")
				return Token{Type: lex_EOF, Loc: l.start}
			}
			tokType := lex_IDENT
			if l.state == stateXUI {
				tokType = lex_UIDENT
			}
			return Token{Type: tokType, Str: str, Loc: l.start}
		}

		l.literalbuf.WriteByte(ch)
		l.pos++
	}

	l.Err = fmt.Errorf("unterminated quoted identifier")
	l.state = stateInitial
	return Token{Type: lex_EOF, Loc: l.start}
}

// lexDollarQuote handles $tag$...$tag$ strings.
func (l *Lexer) lexDollarQuote() Token {
	for l.pos < len(l.input) {
		ch := l.input[l.pos]

		if ch == '$' {
			// Check for ending delimiter
			endTag := l.peekDollarTag()
			if endTag == l.dolqstart {
				l.pos += len(endTag)
				l.state = stateInitial
				return Token{Type: lex_SCONST, Str: l.literalbuf.String(), Loc: l.start}
			}
		}

		l.literalbuf.WriteByte(ch)
		l.pos++
	}

	l.Err = fmt.Errorf("unterminated dollar-quoted string")
	l.state = stateInitial
	return Token{Type: lex_EOF, Loc: l.start}
}

// scanDollarTag scans and returns a dollar tag ($...$) if present.
func (l *Lexer) scanDollarTag() string {
	if l.pos >= len(l.input) || l.input[l.pos] != '$' {
		return ""
	}

	start := l.pos
	l.pos++ // skip first $

	// Tag can be empty ($$ ... $$) or have identifier chars
	for l.pos < len(l.input) {
		ch := l.input[l.pos]
		if ch == '$' {
			l.pos++
			return l.input[start:l.pos]
		}
		if l.pos == start+1 {
			// First char must be letter or underscore
			if !isIdentStart(ch) {
				l.pos = start
				return ""
			}
		} else {
			// Subsequent chars can include digits
			if !isIdentCont(ch) {
				l.pos = start
				return ""
			}
		}
		l.pos++
	}

	l.pos = start
	return ""
}

// peekDollarTag peeks at a dollar tag without advancing position.
func (l *Lexer) peekDollarTag() string {
	savedPos := l.pos
	tag := l.scanDollarTag()
	l.pos = savedPos
	return tag
}

// lexParam handles parameter tokens ($1, $2, etc.)
func (l *Lexer) lexParam() Token {
	l.pos++ // skip $
	start := l.pos
	for l.pos < len(l.input) && isDigit(l.input[l.pos]) {
		l.pos++
	}

	// Check for trailing junk
	if l.pos < len(l.input) && isIdentStart(l.input[l.pos]) {
		l.Err = fmt.Errorf("trailing junk after parameter")
		return Token{Type: lex_EOF, Loc: l.start}
	}

	val, _ := strconv.ParseInt(l.input[start:l.pos], 10, 64)
	return Token{Type: lex_PARAM, Ival: val, Loc: l.start}
}

// lexOperator handles operator tokens.
func (l *Lexer) lexOperator() Token {
	start := l.pos

	for l.pos < len(l.input) && isOpChar(l.input[l.pos]) {
		l.pos++
	}

	op := l.input[start:l.pos]

	// Check for embedded comment start
	if idx := strings.Index(op, "/*"); idx > 0 {
		l.pos = start + idx
		op = op[:idx]
	}
	if idx := strings.Index(op, "--"); idx > 0 && (idx < len(op)-1 || idx < strings.Index(op, "/*")) {
		if strings.Index(op, "/*") < 0 || idx < strings.Index(op, "/*") {
			l.pos = start + idx
			op = op[:idx]
		}
	}

	// For SQL compatibility, strip trailing + or - unless preceded by special chars
	for len(op) > 1 && (op[len(op)-1] == '+' || op[len(op)-1] == '-') {
		hasSpecial := false
		for i := len(op) - 2; i >= 0; i-- {
			c := op[i]
			if c == '~' || c == '!' || c == '@' || c == '#' || c == '^' ||
				c == '&' || c == '|' || c == '`' || c == '?' || c == '%' {
				hasSpecial = true
				break
			}
		}
		if hasSpecial {
			break
		}
		op = op[:len(op)-1]
		l.pos--
	}

	// Single-char ops that should be returned as their char value
	if len(op) == 1 && isSelf(op[0]) {
		return Token{Type: int(op[0]), Str: op, Loc: l.start}
	}

	// Two-char special tokens
	if len(op) == 2 {
		switch op {
		case "=>":
			return Token{Type: lex_EQUALS_GREATER, Str: op, Loc: l.start}
		case ">=":
			return Token{Type: lex_GREATER_EQUALS, Str: op, Loc: l.start}
		case "<=":
			return Token{Type: lex_LESS_EQUALS, Str: op, Loc: l.start}
		case "<>":
			return Token{Type: lex_NOT_EQUALS, Str: op, Loc: l.start}
		case "!=":
			return Token{Type: lex_NOT_EQUALS, Str: op, Loc: l.start}
		}
	}

	if len(op) >= 63 { // NAMEDATALEN
		l.Err = fmt.Errorf("operator too long")
		return Token{Type: lex_EOF, Loc: l.start}
	}

	return Token{Type: lex_Op, Str: op, Loc: l.start}
}

// lexNumber handles integer and floating-point literals.
func (l *Lexer) lexNumber() Token {
	start := l.pos

	// Check for hex, octal, or binary prefix
	if l.input[l.pos] == '0' && l.pos+1 < len(l.input) {
		next := l.input[l.pos+1]
		if next == 'x' || next == 'X' {
			return l.lexHexNumber()
		}
		if next == 'o' || next == 'O' {
			return l.lexOctalNumber()
		}
		if next == 'b' || next == 'B' {
			return l.lexBinaryNumber()
		}
	}

	// Decimal integer or numeric
	l.scanDecDigits()

	isFloat := false

	// Check for decimal point
	if l.pos < len(l.input) && l.input[l.pos] == '.' {
		// Check for .. (DOT_DOT)
		if l.pos+1 < len(l.input) && l.input[l.pos+1] == '.' {
			// Return the integer part, let .. be handled separately
			goto done
		}
		l.pos++
		l.scanDecDigits()
		isFloat = true
	}

	// Check for exponent
	if l.pos < len(l.input) && (l.input[l.pos] == 'e' || l.input[l.pos] == 'E') {
		l.pos++
		if l.pos < len(l.input) && (l.input[l.pos] == '+' || l.input[l.pos] == '-') {
			l.pos++
		}
		if l.pos >= len(l.input) || !isDigit(l.input[l.pos]) {
			l.Err = fmt.Errorf("trailing junk after numeric literal")
			return Token{Type: lex_EOF, Loc: l.start}
		}
		l.scanDecDigits()
		isFloat = true
	}

done:
	// Check for trailing identifier
	if l.pos < len(l.input) && isIdentStart(l.input[l.pos]) {
		l.Err = fmt.Errorf("trailing junk after numeric literal")
		return Token{Type: lex_EOF, Loc: l.start}
	}

	numStr := l.input[start:l.pos]
	// Remove underscores for parsing
	numStr = strings.ReplaceAll(numStr, "_", "")

	if isFloat {
		return Token{Type: lex_FCONST, Str: numStr, Loc: l.start}
	}

	// Try to parse as integer
	val, err := strconv.ParseInt(numStr, 10, 64)
	if err != nil {
		// Too large, treat as float
		return Token{Type: lex_FCONST, Str: numStr, Loc: l.start}
	}

	return Token{Type: lex_ICONST, Ival: val, Str: numStr, Loc: l.start}
}

// scanDecDigits scans decimal digits with optional underscores.
func (l *Lexer) scanDecDigits() {
	for l.pos < len(l.input) {
		ch := l.input[l.pos]
		if isDigit(ch) || ch == '_' {
			l.pos++
		} else {
			break
		}
	}
}

// lexHexNumber handles 0x... hex integers.
func (l *Lexer) lexHexNumber() Token {
	start := l.pos
	l.pos += 2 // skip 0x

	if l.pos >= len(l.input) || (!isHexDigit(l.input[l.pos]) && l.input[l.pos] != '_') {
		l.Err = fmt.Errorf("invalid hexadecimal integer")
		return Token{Type: lex_EOF, Loc: l.start}
	}

	for l.pos < len(l.input) && (isHexDigit(l.input[l.pos]) || l.input[l.pos] == '_') {
		l.pos++
	}

	if l.pos < len(l.input) && isIdentStart(l.input[l.pos]) {
		l.Err = fmt.Errorf("trailing junk after numeric literal")
		return Token{Type: lex_EOF, Loc: l.start}
	}

	numStr := l.input[start:l.pos]
	numStr = strings.ReplaceAll(numStr, "_", "")

	val, err := strconv.ParseInt(numStr[2:], 16, 64)
	if err != nil {
		return Token{Type: lex_FCONST, Str: numStr, Loc: l.start}
	}

	return Token{Type: lex_ICONST, Ival: val, Str: numStr, Loc: l.start}
}

// lexOctalNumber handles 0o... octal integers.
func (l *Lexer) lexOctalNumber() Token {
	start := l.pos
	l.pos += 2 // skip 0o

	if l.pos >= len(l.input) || (!isOctalDigit(l.input[l.pos]) && l.input[l.pos] != '_') {
		l.Err = fmt.Errorf("invalid octal integer")
		return Token{Type: lex_EOF, Loc: l.start}
	}

	for l.pos < len(l.input) && (isOctalDigit(l.input[l.pos]) || l.input[l.pos] == '_') {
		l.pos++
	}

	if l.pos < len(l.input) && isIdentStart(l.input[l.pos]) {
		l.Err = fmt.Errorf("trailing junk after numeric literal")
		return Token{Type: lex_EOF, Loc: l.start}
	}

	numStr := l.input[start:l.pos]
	numStr = strings.ReplaceAll(numStr, "_", "")

	val, err := strconv.ParseInt(numStr[2:], 8, 64)
	if err != nil {
		return Token{Type: lex_FCONST, Str: numStr, Loc: l.start}
	}

	return Token{Type: lex_ICONST, Ival: val, Str: numStr, Loc: l.start}
}

// lexBinaryNumber handles 0b... binary integers.
func (l *Lexer) lexBinaryNumber() Token {
	start := l.pos
	l.pos += 2 // skip 0b

	if l.pos >= len(l.input) || (!isBinaryDigit(l.input[l.pos]) && l.input[l.pos] != '_') {
		l.Err = fmt.Errorf("invalid binary integer")
		return Token{Type: lex_EOF, Loc: l.start}
	}

	for l.pos < len(l.input) && (isBinaryDigit(l.input[l.pos]) || l.input[l.pos] == '_') {
		l.pos++
	}

	if l.pos < len(l.input) && isIdentStart(l.input[l.pos]) {
		l.Err = fmt.Errorf("trailing junk after numeric literal")
		return Token{Type: lex_EOF, Loc: l.start}
	}

	numStr := l.input[start:l.pos]
	numStr = strings.ReplaceAll(numStr, "_", "")

	val, err := strconv.ParseInt(numStr[2:], 2, 64)
	if err != nil {
		return Token{Type: lex_FCONST, Str: numStr, Loc: l.start}
	}

	return Token{Type: lex_ICONST, Ival: val, Str: numStr, Loc: l.start}
}

// lexIdent handles identifiers and keywords.
func (l *Lexer) lexIdent() Token {
	start := l.pos

	// Scan identifier
	for l.pos < len(l.input) && isIdentCont(l.input[l.pos]) {
		l.pos++
	}

	ident := l.input[start:l.pos]

	// Check if it's a keyword
	kw := LookupKeyword(ident)
	if kw != nil {
		return Token{Type: kw.Token, Str: kw.Name, Loc: l.start}
	}

	// Convert to lowercase for non-keywords
	ident = l.downcase(ident)

	// Truncate if too long
	if len(ident) >= 63 { // NAMEDATALEN
		ident = ident[:63]
	}

	return Token{Type: lex_IDENT, Str: ident, Loc: l.start}
}

// downcase converts an identifier to lowercase.
func (l *Lexer) downcase(s string) string {
	return strings.ToLower(s)
}

// Character classification functions

func isDigit(ch byte) bool {
	return ch >= '0' && ch <= '9'
}

func isHexDigit(ch byte) bool {
	return (ch >= '0' && ch <= '9') || (ch >= 'a' && ch <= 'f') || (ch >= 'A' && ch <= 'F')
}

func isOctalDigit(ch byte) bool {
	return ch >= '0' && ch <= '7'
}

func isBinaryDigit(ch byte) bool {
	return ch == '0' || ch == '1'
}

func hexValue(ch byte) int {
	if ch >= '0' && ch <= '9' {
		return int(ch - '0')
	}
	if ch >= 'a' && ch <= 'f' {
		return int(ch - 'a' + 10)
	}
	return int(ch - 'A' + 10)
}

func isIdentStart(ch byte) bool {
	// A-Z, a-z, _, and high bytes (for UTF-8)
	return (ch >= 'A' && ch <= 'Z') || (ch >= 'a' && ch <= 'z') || ch == '_' || ch >= 128
}

func isIdentCont(ch byte) bool {
	// A-Z, a-z, 0-9, _, $, and high bytes
	return isIdentStart(ch) || (ch >= '0' && ch <= '9') || ch == '$'
}

func isSelf(ch byte) bool {
	// Characters that are self-delimiting tokens
	return ch == ',' || ch == '(' || ch == ')' || ch == '[' || ch == ']' ||
		ch == '.' || ch == ';' || ch == ':' || ch == '+' || ch == '-' ||
		ch == '*' || ch == '/' || ch == '%' || ch == '^' || ch == '<' ||
		ch == '>' || ch == '='
}

func isSelfOnly(ch byte) bool {
	// Characters that are ONLY self-delimiting (not part of operators)
	return ch == ',' || ch == '(' || ch == ')' || ch == '[' || ch == ']' ||
		ch == '.' || ch == ';' || ch == ':'
}

func isOpChar(ch byte) bool {
	// Characters that can appear in operators
	return ch == '~' || ch == '!' || ch == '@' || ch == '#' || ch == '^' ||
		ch == '&' || ch == '|' || ch == '`' || ch == '?' || ch == '+' ||
		ch == '-' || ch == '*' || ch == '/' || ch == '%' || ch == '<' ||
		ch == '>' || ch == '='
}

// Unicode surrogate pair handling

func isUTF16SurrogateFirst(r rune) bool {
	return r >= 0xD800 && r <= 0xDBFF
}

func isUTF16SurrogateSecond(r rune) bool {
	return r >= 0xDC00 && r <= 0xDFFF
}

func surrogateToCodepoint(high, low rune) rune {
	return ((high - 0xD800) << 10) + (low - 0xDC00) + 0x10000
}

// isValidUnicodeCodepoint checks if a code point is valid.
func isValidUnicodeCodepoint(r rune) bool {
	return r <= unicode.MaxRune && !isUTF16SurrogateFirst(r) && !isUTF16SurrogateSecond(r)
}
